{
    "0": {
        "text": "Clemens Biehl, November 2023\nNatural Language Processing\nModul Aktuelle Data Science-Entwicklungen\n1\n",
        "images": [
            {
                "image_xreference": 5,
                "ext": "jpeg"
            }
        ]
    },
    "1": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nWhy Natural Language Processing?\n2\n\u2022 Natural Language Interfaces\n",
        "images": [
            {
                "image_xreference": 17,
                "ext": "png"
            }
        ]
    },
    "2": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Natural Language Interfaces\nWhy Natural Language Processing?\n3\nhttps://rasa.com/docs/\nSounds like intent \n\u201eask_weather\u201c! \nDate is \u201e24.03.21\u201c\nOk, for \n\u201eask_weather\u201c I \nmust ask the weather \ndatabase!\n20\u00b0, sunny!\nOk, I will put this \ninto a nice natural \nlanguage output\nOh, I got a new \nmessage from \ntelegram\nHey telegram, here\u2019s \nyour response!\n",
        "images": [
            {
                "image_xreference": 24,
                "ext": "png"
            },
            {
                "image_xreference": 25,
                "ext": "png"
            }
        ]
    },
    "3": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nWhy Natural Language Processing?\n\u2022 Search Engines\n4\n",
        "images": [
            {
                "image_xreference": 33,
                "ext": "png"
            },
            {
                "image_xreference": 34,
                "ext": "png"
            }
        ]
    },
    "4": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nWhy Natural Language Processing?\n\u2022 Machine Translation\n5\nEuroparl:  \nA Parallel Corpus for Statistical Machine Translation,  \nPhilipp Koehn, MT Summit 2005\nEncoder-Decoder Neural Network\nStochastic gradient descent\n\u201eBeaucoup des pommes rouges\u201c\n\u201eViele rote \u00c4pfel\u201c\n",
        "images": [
            {
                "image_xreference": 41,
                "ext": "png"
            },
            {
                "image_xreference": 45,
                "ext": "png"
            }
        ]
    },
    "5": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nWhy Natural Language Processing?\n6\n\u2022 Bots and Fake News\n",
        "images": [
            {
                "image_xreference": 54,
                "ext": "png"
            }
        ]
    },
    "6": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nLearning Goals\n7\nAfter taking this course, you will be able to: \n\u2022 \u2026understand and implement natural language processing pipelines for \nvarious use cases \n\u2022 \u2026represent natural language in di\ufb00erent ways and decide when to use which \nkind of representation \n\u2022 \u2026implement text classi\ufb01ers, sequence taggers and chatbots \n\u2022 \u2026make an informed decision on which tools and frameworks to use\n",
        "images": []
    },
    "7": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nCourse Organization\n\u2022 Weekly lectures and exercise sessions (cf. Google calendar) \n\u2022 Questions can be asked anytime on Moodle or in the weekly exercise \nsessions \n\u2022 Contact: cbiehl.teaching@icloud.com (please use Moodle if possible)\n8\n",
        "images": []
    },
    "8": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nGrading & Projects\n\u2022 You will work on NLP projects in groups of 4-6 people \n\u2022 The projects will be organized via a Github repository:\u2028\nhttps://github.com/cbiehl/wwi21-nlp \n\u2022 You can \ufb01nd proposals for project topics in the ReadMe on Github, but your own \nproposals are welcome (deadline: next lecture)! \n\u2022 Please create one Github issue per group and state the following info: \n- Project topic: \u2028\nPlease give the issue a reasonable title and describe your project idea in detail in the \nissue \n- Names of all group members (can also be handled via email) \n- Link to your group\u2019s Github repository for the project source code\u2028\n(using Github is not mandatory but only a suggestion)\n9\n",
        "images": []
    },
    "9": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nGrading & Projects\n\u2022 Example topics from previous years: \n\u2022 Music recommender system based on song lyrics \n\u2022 Fake news classi\ufb01cation \n\u2022 Chatbot supporting students on learning for exams \n\u2022 Phishing email detection \n\u2022 Movie recommendation chatbot \n\u2022 Sentiment analysis on Twitter, Reddit and news\n10\n",
        "images": []
    },
    "10": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nGrading & Projects\n\u2022 Each group presents their project (with live demo if appropriate) in the \ufb01nal exercise \nsession \n\u2022 Grading will be based on the quality, originality and scope of your project and the \n\ufb01nal presentation \n\u2022 You must submit: \n\u2022 Your code (incl. short instruction on how to run it) \n\u2022 Your \ufb01nal presentation as PDF \ufb01le \n\u2022 A report of 2 to 4 pages describing your project in detail as PDF \ufb01le \n\u2022 Plagiarism will not be tolerated!\u2028\n\u2014> It is okay to use existing tools and techniques, but not to copy without\u2028\n       stating the source or to hand in the same solution twice.\n11\n",
        "images": []
    },
    "11": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nGrading & Projects\n\u2022 Requirements for your projects: \n- 4-6 people per group \n- End result must be working software\u2028\n\u2014> You can build an application or website for your NLP models or \u2028\n       you can do an in-depth evaluation of di\ufb00erent models for a task \n- The topic must be related to NLP and focused on NLP \n- You must make your process and results transparent in the \ufb01nal report and presentation: \n\u2022 What was the goal of your project? \n\u2022 Which data sources and models did you use? Show the creative and challenging parts of your \nimplementation! \n\u2022 How does your model work? (It\u2019s okay to use existing libraries, but you need to understand the model.) \n\u2022 How did you evaluate your models? What are the results? What are possible future improvements?\n12\n",
        "images": []
    },
    "12": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nGrading & Projects\n\u2022 Project report: \n- Report must be written in a scienti\ufb01c style\u2028\n(abstract, introduction, related work, description of the method or software \narchitecture, discussion, conclusion) \n- LaTeX Template: https://www.overleaf.com/latex/templates/web-of-conferences-a4-\npaper-size-two-columns-format/qgcdqrqhwqbr \n- 2 to 4 pages \n- Present related work (previous or similar approaches in open source, literature or \nelsewhere) and highlight your own contributions \n- You must state which project members contributed to what\n13\n",
        "images": []
    },
    "13": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nContents\n1. Overview, Applications and Challenges \n2. NLP Pipelines \n3. Vector Semantics and Embeddings \n4. Language Models \n5. Text Classi\ufb01cation and Sentiment Analysis \n6. RNNs and Transformers for Sequence Tagging \n7. Seq2Seq Neural Networks and Machine Translation \n8. Chatbots and Dialogue Systems\n14\n",
        "images": []
    },
    "14": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nRecommended Literature\n\u2022 \u201eSpeech and Language Processing\u201c, Dan Jurafsky and James H. Martin\u2028\nhttps://web.stanford.edu/~jurafsky/slp3/ \u2028\n\u2022 \u201eNatural Language Processing\u201c, Jacob Eisenstein\u2028\nhttps://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf \u2028\n\u2022 \u201eLanguage Processing and Python\u201c\u2028\nhttp://www.nltk.org/book/ \u2028\n\u2022 \u201eA Primer on Neural Networks for Language Processing\u201c, Yoav Goldberg\u2028\nhttps://arxiv.org/abs/1510.00726 \u2028\n\u2022 Stanford CS224N Lecture\u2028\nhttps://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4 \u2028\n\u2022 HuggingFace Transformers Course\u2028\nhttps://huggingface.co/course/chapter1/1 \n15\n",
        "images": []
    },
    "15": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Natural language processing (NLP) is a set of methods for making human \nlanguage accessible to computers \n\u2022 At the intersection of many other \ufb01elds:\nWhat is \u201eNLP\u201c?\n16\nLinguistics\nComputer Science\nMachine Learning\nSpeech Processing\nEthics\n",
        "images": []
    },
    "16": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Variety \n\u2022 Ambiguity \n\u2022 Polysemy \n\u2022 World knowledge, context \n\u2022 Memory \n\u2022 Multi-modality \n\u2022 Many di\ufb00erent languages and cultures \n\u2022 Domain speci\ufb01cs \n\u2022 \u2026\nWhy NLP is hard\n17\n",
        "images": []
    },
    "17": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Variety\u2028\n\u2028\nWhy NLP is hard\n18\nThis movie was awesome\nI really enjoyed this \ufb01lm\nThe director did a pretty \ngood job\nWhat a beatuiful plot\nTypos!\nThat was thrilling\nI could hardly write a better \nstory than this one\n+ irony, sarcasm, etc.\n",
        "images": []
    },
    "18": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Ambiguity\nWhy NLP is hard\n19\n\u201eScientists study whales from space.\u201c\n",
        "images": [
            {
                "image_xreference": 123,
                "ext": "png"
            }
        ]
    },
    "19": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Polysemy / Ambiguity\nWhy NLP is hard\n20\n\u201eI know Florence well.\u201c\n?\n",
        "images": [
            {
                "image_xreference": 128,
                "ext": "png"
            }
        ]
    },
    "20": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Polysemy / Ambiguity\n21\n\u201eStaubecken\u201c\n?\nWhy NLP is hard\n",
        "images": [
            {
                "image_xreference": 133,
                "ext": "png"
            },
            {
                "image_xreference": 134,
                "ext": "png"
            }
        ]
    },
    "21": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Humans don\u2019t always act as intended\u2026\nWhy NLP is hard\n22\n",
        "images": [
            {
                "image_xreference": 139,
                "ext": "png"
            }
        ]
    },
    "22": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nHow can we approach NLP?\n23\nLanguage is compositional, smaller units combine to form larger ones \n",
        "images": [
            {
                "image_xreference": 144,
                "ext": "png"
            }
        ]
    },
    "23": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nHow can we approach NLP?\n24\nLanguage is compositional, smaller units combine to form larger ones \n\u2022 Di\ufb00erent disciplines in computational linguistics are concerned with di\ufb00erent levels: \n- Phonetics/Phonology \n- Segmentation (What is a word? How to separate text into single words?) \n- Morphology (Word forms and word formation, smallest units of meaning) \n- Syntax (How are sentences and larger structures formed from smaller units?) \n- Semantics (Study of meaning of words, phrases, sentences or documents) \n- Discourse and Pragmatics (What is the purpose of an utterance? Which references are made?)\n",
        "images": []
    },
    "24": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Language can be parsed into  \n\u2022 structured representations\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2028\n\u2022 vector representations\nHow can we approach NLP?\n25\nWhale\nDolphin\nFish\nScientist\nWoman\nMan\nWhale =\n0.0241\n0.0018\n0.1092\n\u22ee\n\u2208 \u211dM\n",
        "images": [
            {
                "image_xreference": 157,
                "ext": "png"
            },
            {
                "image_xreference": 158,
                "ext": "png"
            }
        ]
    },
    "25": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Transfer Learning (ELMo, BERT, GPT, \u2026)\u2028\n\u2014> good representation learning enables transfer learning and zero-shot learning \n\u2022 Multi-Task Learning (T5)\u2028\n\u2014> models can learn useful shared representations between multiple tasks \n\u2022 E\ufb03cient NLP\u2028\n\u2014> more research is done on how to keep the good performance of large\u2028\n       transformer models but reduce the computational cost \n\u2022 Human-in-the-Loop\u2028\n\u2014> as more and more systems are deployed in production, the need for\u2028\n       e\ufb03cient human feedback loops increases \n\u2022 Bias in NLP models\u2028\n\u2014> more research is done on which biases NLP models contain and how to \u2028\n       \u201ede-bias\u201c them\nRecent Advances\n26\n",
        "images": []
    },
    "26": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nNext Up: NLP Pipelines\n27\nTokenization,  \nNormalization, \nPart-of-Speech Tagging,  \nNamed Entity Recognition,  \nSyntactic Parsing \n& More\n",
        "images": []
    },
    "27": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\nMeme of the Day\n28\n",
        "images": [
            {
                "image_xreference": 175,
                "ext": "png"
            }
        ]
    },
    "28": {
        "text": "Natural Language Processing\n2023/24, DHBW Mannheim\n\u2022 Eisenstein, Chapter 1 - Introduction\u2028\nhttps://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/\neisenstein-nlp-notes.pdf\nRecommended Readings\n29\n",
        "images": []
    }
}