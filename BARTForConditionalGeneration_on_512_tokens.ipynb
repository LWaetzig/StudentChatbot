{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6iAiVwtqjS",
        "outputId": "381aef0b-8ab4-4f30-af57-fbcdfb65ba40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.15-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.15 PyMuPDFb-1.23.9\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=c7a658fee31295499a266a625fa5277d896b688f04067985459ceff233a033b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.1)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=2e8284a7d6d8aa36740d48e43d04b1e1aaae1da1461239b170ea71d89d3868c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.13 (from langchain)\n",
            "  Downloading langchain_community-0.0.13-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.9 (from langchain)\n",
            "  Downloading langchain_core-0.1.11-py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.81-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.1 langchain-community-0.0.13 langchain-core-0.1.11 langsmith-0.0.81 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install PyMuPDF\n",
        "!pip install langdetect\n",
        "!pip install rake-nltk\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "uLMQiuAwt1-E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "6SfRg37L4hu9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# load bart tokenizer and model from huggingface\n",
        "tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\n",
        "generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa').to(device)\n"
      ],
      "metadata": {
        "id": "vw5YPxhQt2n8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging as logger\n",
        "import math\n",
        "import os\n",
        "import sqlite3\n",
        "from pprint import pprint\n",
        "\n",
        "import fitz\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from langdetect import detect\n",
        "from rake_nltk import Rake\n",
        "from scipy.signal import argrelextrema\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import nltk\n",
        "nltk.download(['stopwords', 'punkt'])\n",
        "from transformers import AutoTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "logger.basicConfig(level=logger.INFO)\n",
        "# TODO: implement function to get matched documents based in user prompt\n",
        "# TODO: add logging to file\n",
        "# TODO: check for syntax and formatting\n",
        "# TODO: add comments\n",
        "# TODO: implement method to preprocess also .txt files (currently only pdf, compare line 35)\n",
        "# TODO: fix the bug that the filename is currently not written to db (compare line 288)\n",
        "\n",
        "\n",
        "class FileProcessor:\n",
        "    def __init__(self, file):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        logger.info(\"Initializing FileProcessor\")\n",
        "        self.document = None\n",
        "        # read in pdf document\n",
        "        try:\n",
        "            file_content = file.getvalue()\n",
        "            self.document = fitz.open(\"pdf\", file_content)\n",
        "            logger.info(\"PDF document loaded\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Could not open PDF file: {e}\")\n",
        "        self.document_content = {\"text\": str(), \"images\": dict()}\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text(\n",
        "        document: fitz.fitz.Document, extract_text_from_image: bool = False\n",
        "    ) -> str:\n",
        "        \"\"\"extract text from a pdf page\n",
        "        Args:\n",
        "            page (fitz.fitz.Page): page from pdf document\n",
        "            extract_from_image (bool, optional): extract text from image using OCR (not possible yet). Defaults to False.\n",
        "        Returns:\n",
        "            str: text from page\n",
        "        \"\"\"\n",
        "\n",
        "        if extract_text_from_image:\n",
        "            logger.info(\"Extracting text from image\")\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            logger.info(\"Extracting text from pages\")\n",
        "            text = str()\n",
        "            for page in document:\n",
        "                try:\n",
        "                    text += page.get_text() + \" \"\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file on page {page.number}: {e}\")\n",
        "                    continue\n",
        "            return text\n",
        "    @staticmethod\n",
        "    def calc_rev_sigmoid(x: float) -> float:\n",
        "        \"\"\"calculate reverse sigmoid function\n",
        "\n",
        "        Args:\n",
        "            x (float): input value\n",
        "\n",
        "        Returns:\n",
        "            float: output value\n",
        "        \"\"\"\n",
        "        return 1 / (1 + math.exp(0.5 * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def activate_similarities(similarities: np.array, p_size=10) -> np.array:\n",
        "        \"\"\"calculate activated similarities using reverse sigmoid function\n",
        "\n",
        "        Args:\n",
        "            similarities (np.array): similarities between sentences\n",
        "            p_size (int, optional): size of sigmoid function. Defaults to 10.\n",
        "\n",
        "        Returns:\n",
        "            np.array: activated similarities\n",
        "        \"\"\"\n",
        "        x = np.linspace(-10, 10, p_size)\n",
        "        y = np.vectorize(FileProcessor.calc_rev_sigmoid)\n",
        "        activation_weights = np.pad(y(x), (0, similarities.shape[0] - p_size))\n",
        "        diagonals = [\n",
        "            similarities.diagonal(each) for each in range(0, similarities.shape[0])\n",
        "        ]\n",
        "        diagonals = [\n",
        "            np.pad(each, (0, similarities.shape[0] - len(each))) for each in diagonals\n",
        "        ]\n",
        "        diagonals = np.stack(diagonals)\n",
        "        diagonals = diagonals * activation_weights.reshape(-1, 1)\n",
        "        activated_similarities = np.sum(diagonals, axis=0)\n",
        "        return activated_similarities\n",
        "\n",
        "\n",
        "    def split_text_into_chunks(\n",
        "      self,\n",
        "      text: str,\n",
        "      filename: str,\n",
        "      visualize_splitting: bool = False) -> list:\n",
        "      doc_lang = detect(text)\n",
        "      stop_words = set(stopwords.words('german')) if doc_lang == \"de\" else set(stopwords.words('english'))\n",
        "\n",
        "      rake = Rake(stopwords=stop_words)\n",
        "\n",
        "      logger.info(\"Loading model\")\n",
        "      model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "      sentences = text.split(\". \")\n",
        "\n",
        "      logger.info(\"Encoding sentences\")\n",
        "      embeddings = model.encode(sentences)\n",
        "      similarities = cosine_similarity(embeddings, embeddings)\n",
        "      logger.info(\"Calculating activated similarities\")\n",
        "      activated_similarities = self.activate_similarities(similarities, p_size=5)\n",
        "      minima = argrelextrema(activated_similarities, np.less, order=2)\n",
        "      split_points = [each for each in minima[0]]\n",
        "\n",
        "      if visualize_splitting:\n",
        "          self.plot_chunk_points(activated_similarities, split_points)\n",
        "\n",
        "      logger.info(\"Creating chunks list\")\n",
        "      chunks = []\n",
        "      text_chunk = str()\n",
        "      for split_point, sentence in enumerate(sentences):\n",
        "          text_chunk += sentence + \". \"\n",
        "          if split_point in split_points:\n",
        "              rake.extract_keywords_from_text(text_chunk)\n",
        "              extracted_keywords = rake.get_ranked_phrases()[:5]\n",
        "              chunks.append(text_chunk)\n",
        "              text_chunk = str()\n",
        "      if text_chunk != str():\n",
        "          rake.extract_keywords_from_text(text_chunk)\n",
        "          chunks.append(text_chunk)\n",
        "\n",
        "      final_chunks = []\n",
        "      for chunk in chunks:\n",
        "            if len(self.tokenizer.tokenize(chunk)) > 512:\n",
        "                final_chunks.extend(self.divide_and_conquer(chunk))\n",
        "            else:\n",
        "                final_chunks.append(chunk)\n",
        "\n",
        "      return final_chunks\n",
        "\n",
        "\n",
        "    def divide_and_conquer(self, chunk: str) -> list:\n",
        "      sentences = nltk.sent_tokenize(chunk)\n",
        "      new_chunk = \"\"\n",
        "      sub_chunks = []\n",
        "\n",
        "      for sentence in sentences:\n",
        "          # Wenn der aktuelle Satz zu lang ist, aufteilen\n",
        "          if len(self.tokenizer.tokenize(sentence)) > 512:\n",
        "              # Teilen des langen Satz in kleinere Teile\n",
        "              sub_chunks.extend(self.divide_long_sentence(sentence))\n",
        "          elif len(self.tokenizer.tokenize(new_chunk + sentence)) > 512:\n",
        "              sub_chunks.append(new_chunk.strip())\n",
        "              new_chunk = sentence\n",
        "          else:\n",
        "              new_chunk += \" \" + sentence\n",
        "\n",
        "      if new_chunk:\n",
        "          sub_chunks.append(new_chunk.strip())\n",
        "\n",
        "      return sub_chunks\n",
        "\n",
        "    def divide_long_sentence(self, sentence: str) -> list:\n",
        "      words = sentence.split()\n",
        "      sub_sentences = []\n",
        "      current_chunk = []\n",
        "\n",
        "      for word in words:\n",
        "          current_chunk.append(word)\n",
        "          # Prüfen, ob die Länge des aktuellen Chunks die Grenze erreicht hat\n",
        "          if len(self.tokenizer.tokenize(' '.join(current_chunk) + ' ' + word)) > 512:\n",
        "              sub_sentences.append(' '.join(current_chunk))\n",
        "              current_chunk = []\n",
        "\n",
        "      # Fügen letzten Chunk hinzu, falls vorhanden\n",
        "      if current_chunk:\n",
        "          sub_sentences.append(' '.join(current_chunk))\n",
        "\n",
        "      return sub_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Hxh8JB5JZE",
        "outputId": "26c54403-4a81-4335-bab2-b247eefa04af"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF-Dokument öffnen\n",
        "with fitz.open('Databases - 10-IndexingHashing.pdf') as doc:\n",
        "  processor = FileProcessor(doc)\n",
        "  text = processor.extract_text(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8wSAt1K576W",
        "outputId": "c3759865-a1e7-4614-9135-527b07ffeeb9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Could not open PDF file: 'Document' object has no attribute 'getvalue'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Databases - 10-IndexingHashing.pdf'\n",
        "chunks= processor.split_text_into_chunks(text, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbC2BH7z5-3a",
        "outputId": "4af9899d-17c7-41cd-a5e3-f542b3abd630"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3865 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_query(query, context):\n",
        "    # contcatinate the query and context passages\n",
        "    query = f\"question: {query} context: {context}\"\n",
        "    return query"
      ],
      "metadata": {
        "id": "_8-rAj4d34-3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=format_query(\"What kind of secondary indexes exist?\", chunks[5])\n",
        "pprint(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F_L8tgj50gM",
        "outputId": "2b038c51-c650-4cac-ff14-51e5e0d83101"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('question: What kind of secondary indexes exist? context: A secondary index '\n",
            " 'which is created on a key (unique) ﬁeld\\n'\n",
            " '(secondary key) has one index entry for each record\\n'\n",
            " '–\\xa0represents dense index and has pointer to the block of the record or\\n'\n",
            " 'to the record itself\\n'\n",
            " '–\\xa0this corresponds to any UNIQUE key attribute\\n'\n",
            " '2. A secondary index which is created on a non-key ﬁeld can have for\\n'\n",
            " 'each index entry (indexing ﬁeld) more corresponding records in the\\n'\n",
            " 'data ﬁle\\n'\n",
            " '–\\xa0pointer of the index entry has address of the location of the block\\n'\n",
            " 'with record pointers to the actual records in the data ﬁle\\n'\n",
            " '–\\xa0represents sparse secondary index\\n'\n",
            " '17\\n'\n",
            " ' Secondary index on a key ﬁeld\\n'\n",
            " 'Secondary index on a key ﬁeld\\n'\n",
            " 'Dense secondary index on a secondary key attribute\\n'\n",
            " '18\\n'\n",
            " ' Secondary index on a non-key ﬁeld\\n'\n",
            " 'Secondary index on a non-key ﬁeld\\n'\n",
            " 'Sparse secondary index on a non-key attribute\\n'\n",
            " '19\\n'\n",
            " ' Searching for a record in ﬁle - example\\n'\n",
            " 'Searching for a record in ﬁle - example\\n'\n",
            " 'Suppose that we have a ﬁle with records of ﬁxed size:\\n'\n",
            " '–\\xa0number of records \\n'\n",
            " '–\\xa0record size \\n'\n",
            " ' bytes\\n'\n",
            " '–\\xa0block size \\n'\n",
            " ' bytes\\n'\n",
            " '–\\xa0block organization is unspanned\\n'\n",
            " 'Calculate the cost of the search operation on a non-ordering unique ﬁeld\\n'\n",
            " '(secondary key) with no index used. –\\xa0\\n'\n",
            " ' - blocking factor\\n'\n",
            " '–\\xa0necessary number of blocks in the ﬁle \\n'\n",
            " '–\\xa0only linear search is possible on the non-ordering ﬁeld and it requires '\n",
            " 'on average\\n'\n",
            " ' block accesses\\n'\n",
            " '–\\xa0in the worse case, if such record does not exist in the ﬁle, searching '\n",
            " 'takes \\n'\n",
            " ' accesses\\n'\n",
            " '–\\xa0complexity of the operation is \\n'\n",
            " ' - n number of disk blocks ( )\\n'\n",
            " 'r = 30000\\n'\n",
            " 'R = 100\\n'\n",
            " 'B = 1024\\n'\n",
            " 'bfr = ⌊\\n'\n",
            " '⌋ = 10\\n'\n",
            " '1024\\n'\n",
            " '100\\n'\n",
            " 'b = ⌈\\n'\n",
            " '⌉ = ⌈\\n'\n",
            " '⌉ = 3000\\n'\n",
            " 'r\\n'\n",
            " 'bfr\\n'\n",
            " '30000\\n'\n",
            " '10\\n'\n",
            " '=\\n'\n",
            " '= 1500\\n'\n",
            " 'b\\n'\n",
            " '2\\n'\n",
            " '3000\\n'\n",
            " '2\\n'\n",
            " 'b = 3000\\n'\n",
            " 'O(n)\\n'\n",
            " 'b\\n'\n",
            " '20\\n'\n",
            " ' Secondary index - search example\\n'\n",
            " 'Secondary index - search example\\n'\n",
            " 'Suppose that the ﬁle from the last slide has a dense secondary index with '\n",
            " 'the\\n'\n",
            " 'following properties\\n'\n",
            " '–\\xa0key of the index entry is a secondary key of the data ﬁle of length 9 '\n",
            " 'bytes\\n'\n",
            " '–\\xa0pointer ﬁeld is of size 6 bytes\\n'\n",
            " '–\\xa0size of our index record is therefore 15 bytes\\n'\n",
            " '–\\xa0every record of the data ﬁle has its corresponding index entry \\n'\n",
            " ' \\n'\n",
            " ' \\n'\n",
            " ' index records\\n'\n",
            " '–\\xa0index block size: 1024 bytes\\n'\n",
            " 'Calculate the number of block accesses to ﬁnd a record using our secondary\\n'\n",
            " 'index.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "    # tokenize the query to get input_ids\n",
        "    inputs = tokenizer([query], max_length=1024, return_tensors=\"pt\").to(device)\n",
        "    # use generator to predict output ids\n",
        "    ids = generator.generate(inputs[\"input_ids\"], num_beams=2, min_length=20, max_length=40)\n",
        "    # use tokenizer to decode the output ids\n",
        "    answer = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    return pprint(answer)"
      ],
      "metadata": {
        "id": "oapZhpU_5a4X"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm0MrrYF4yEs",
        "outputId": "ab7b40b2-7b19-4a3f-b73b-f84f16912ac7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('There are two kinds of secondary indexes. The first is a \"dense\" secondary '\n",
            " 'index, where each record has its own index entry. The second is a \"sparse\" '\n",
            " 'secondary index')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auch wenn BART bis zu 1024 Tokens verarbeiten kann, funktioniert es zumindest bei den meisten zufaöölig ausgewählten Fragen besser auf 512 Tokens"
      ],
      "metadata": {
        "id": "3JEQzqFjt6PN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}